{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee9e64-953d-4fb5-ae26-fe491827afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-text-splitters langchain pymupdf langchain_ollama psycopg2-binary pgvector pulsar-client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3495892-7ce6-4bb3-8940-b9ca3d7f8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "import io\n",
    "import requests\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import pulsar\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pdf_schema import PDFInfo\n",
    "from pulsar.schema import JsonSchema\n",
    "import signal\n",
    "#import logging\n",
    "#logging.getLogger(\"pulsar\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb1b57-062b-4f33-a320-e87fcfae9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16ceef-26d9-4dd5-a7c8-780ae0ff290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert queries\n",
    "sources_insert = \"INSERT INTO sources (id, source, uri, title, author, summary) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "#\"\"\"INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "#VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n",
    "semantic_insert = \"INSERT INTO semantic_search (id, page, chunk, embedding) VALUES (%s, %s, %s, %s)\"\n",
    "keyword_insert = \"INSERT INTO keyword_search (id, page, ts) VALUES (%s, %s, to_tsvector('english', %s))\"\n",
    "\n",
    "def pdf_retrieve(url):\n",
    "    with BytesIO() as stream_buffer:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to download PDF; received HTTP {response.status_code} from underlying server\")\n",
    "        for chunk in response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
    "            stream_buffer.write(chunk)\n",
    "        doc = pymupdf.open(stream=stream_buffer, filetype=\"pdf\")\n",
    "        return doc\n",
    "\n",
    "def thread_task(shutdown,db,user,password,host,chunk_size=500, chunk_overlap=50):\n",
    "    con = psycopg2.connect(dbname=db, user=user, password=password, host=host)  # Dedicated connection per thread\n",
    "    con.autocommit = True  # Ensure auto-commit mode to avoid locks\n",
    "    pulcli = pulsar.Client('pulsar://pdf-service:6650')   \n",
    "    consumer = pulcli.subscribe(\n",
    "        topic='scraper-output-test',\n",
    "        subscription_name=f\"pdf-process-consumer\",\n",
    "        consumer_type=pulsar.ConsumerType.Shared,\n",
    "        schema=JsonSchema(PDFInfo) )\n",
    "    # Embeddings connection for each thread\n",
    "    embed = OllamaEmbeddings(\n",
    "        model=\"all-minilm\",\n",
    "        base_url =\"http://host.docker.internal:11434\"\n",
    "    )\n",
    "    # LangChain's Recursive Text Splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    "    )\n",
    "    while not shutdown.is_set():\n",
    "        try:\n",
    "            msg = consumer.receive(timeout_millis=5000)\n",
    "            # Try to process message? Not totally sure why this is needed.\n",
    "            try:\n",
    "                pdf_json = msg.value()\n",
    "                #if embed.embed_query('hello') is None:\n",
    "                #    print(\"Ollama not available\")\n",
    "                # Acknowledge successful processing of the message\n",
    "                consumer.acknowledge(msg)\n",
    "                with con.cursor() as cur:\n",
    "                    try:\n",
    "                        key = hashlib.md5(pdf_json.url.encode()).hexdigest()\n",
    "                        pdf_doc = pdf_retrieve(pdf_json.url)\n",
    "                        source = pdf_json.source\n",
    "                        print(f\"Extracted pdf: {pdf_json.title}, number of pages: {len(pdf_doc)}, hash: {key}\")\n",
    "                        sem_data = []\n",
    "                        key_data = []\n",
    "                        for page in pdf_doc:\n",
    "                            key_data.append((key,page.number,page.get_text().replace('\\x00', ''),))\n",
    "                            chunks = text_splitter.split_text(page.get_text())\n",
    "                            embeddings = embed.embed_documents(chunks)\n",
    "                            for chunk_id, (chunk,embedding) in enumerate(zip(chunks,embeddings)):\n",
    "                                sem_data.append((key,page.number,chunk_id,embedding,))\n",
    "                        # Insert records into Postgres\n",
    "                        cur.execute(sources_insert,(key,source,pdf_json.url,pdf_json.title,pdf_json.authors,pdf_json.summary,))\n",
    "                        execute_values(cur,\n",
    "                                       \"INSERT INTO semantic_search (id, page, chunk, embedding) VALUES %s\"\n",
    "                                       ,sem_data\n",
    "                                      )\n",
    "                        execute_values(cur,\n",
    "                                       \"INSERT INTO keyword_search (id, page, ts) VALUES %s\",\n",
    "                                       key_data,\n",
    "                                       template=\"(%s, %s, to_tsvector('english', %s))\"\n",
    "                                      )\n",
    "                        con.commit()\n",
    "                        print(f\"Completed insertion of pdf: {pdf_json.title}, hash: {key}\")\n",
    "                    except psycopg2.Error as e:\n",
    "                        print(f\"Database throwing errors, need to recreate connection: {e}\")\n",
    "                        con.rollback()\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading PDF from url: {pdf_json.url}: {e}\")\n",
    "                        continue  # Skip to the next page\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process message: {e}\")\n",
    "                # Message failed to be processed\n",
    "                consumer.negative_acknowledge(msg)\n",
    "        except Exception:\n",
    "            # We want timeout so we don't get stuck infinitely waiting for new message, need to evaluate shutdown variable every so often\n",
    "            continue\n",
    "    consumer.close()\n",
    "    con.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Postgres connection parameters\n",
    "    host = \"pdf-service\"\n",
    "    user = \"postgres\"\n",
    "    password = \"admin\"\n",
    "    db = \"search\"\n",
    "    def signal_handler(sig, frame):\n",
    "        print('Interrupt, shutting down gracefully...')\n",
    "        shutdown.set()\n",
    "    \n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    shutdown = multiprocessing.Event()\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    \n",
    "    join_list = []\n",
    "    for _ in range(0,cores):\n",
    "        worker = Process(target=thread_task, args=(shutdown,db,user,password,host,))\n",
    "        worker.start()\n",
    "        join_list.append(worker)\n",
    "        \n",
    "    print(f\"Running {cores} workers\") \n",
    "\n",
    "    # while not shutdown.is_set():\n",
    "    #     time.sleep(10)\n",
    "    #     for i in range(len(join_list)):\n",
    "    #         print(f\"Process {i} is running: {join_list[i].is_alive()}\")\n",
    "\n",
    "    \n",
    "    # while not shutdown.is_set():\n",
    "    #     time.sleep(10)\n",
    "    #     # Go through list in reverse since we are deleting elements\n",
    "    #     for i in range(len(join_list) - 1, -1, -1):\n",
    "    #         if not thread.is_alive():\n",
    "    #             join_list[i].join()\n",
    "    #             join_list.pop(i)\n",
    "    #             consumer = pulcli.subscribe(\n",
    "    #                 topic='scraper-output-test',\n",
    "    #                 subscription_name=f\"pdf-process-consumer\",\n",
    "    #                 consumer_type=pulsar.ConsumerType.Shared,\n",
    "    #                 schema=JsonSchema(PDFInfo) )\n",
    "    #             start_worker_thread(consumer,db,user,password,host)\n",
    "                \n",
    "            \n",
    "    for thread in join_list:\n",
    "        thread.join()\n",
    "    time.sleep(1)\n",
    "    print(\"Completed\")\n",
    "    shutdown.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3460021-d0aa-43b3-91bd-2b64ad8dc975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
