{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee9e64-953d-4fb5-ae26-fe491827afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-text-splitters langchain pymupdf langchain_ollama psycopg2-binary pgvector pulsar-client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3495892-7ce6-4bb3-8940-b9ca3d7f8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import multiprocessing\n",
    "from threading import Thread\n",
    "from psycopg2 import pool\n",
    "import time\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "import io\n",
    "import requests\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import pulsar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16ceef-26d9-4dd5-a7c8-780ae0ff290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean shutdown handler\n",
    "import signal\n",
    "from queue import Queue\n",
    "shutdown = Queue()\n",
    "def signal_handler(sig, frame):\n",
    "    print('Interrupt, shutting down gracefully...')\n",
    "    shutdown.put(True)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "# Class into which we'll process messages from Pulsar\n",
    "from pulsar.schema import Record, String, JsonSchema\n",
    "class PDFInfo(Record):\n",
    "    url = String()\n",
    "    title = String()\n",
    "    authors = String()\n",
    "    summary = String()\n",
    "\n",
    "# Postgres connection parameters\n",
    "host = \"pdf-service\"\n",
    "user = \"postgres\"\n",
    "password = \"admin\"\n",
    "db = \"search\"\n",
    "\n",
    "# Insert queries\n",
    "sources_insert = \"INSERT INTO sources (id, uri, title, author, summary) VALUES (%s, %s, %s, %s, %s)\"\n",
    "#\"\"\"INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "#VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n",
    "semantic_insert = \"INSERT INTO semantic_search (id, page, chunk, embedding) VALUES (%s, %s, %s, %s)\"\n",
    "keyword_insert = \"INSERT INTO keyword_search (id, page, ts) VALUES (%s, %s, to_tsvector('english', %s))\"\n",
    "\n",
    "def thread_task(squeue,con,core,chunk_size=500, chunk_overlap=50):\n",
    "    # Embeddings connection for each thread\n",
    "    embed = OllamaEmbeddings(\n",
    "        model=\"all-minilm\",\n",
    "        base_url =\"http://host.docker.internal:11434\"\n",
    "    )\n",
    "    # LangChain's Recursive Text Splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    "    )\n",
    "    # Pulsar consumer for each thread\n",
    "    pulcli = pulsar.Client('pulsar://pdf-service:6650')\n",
    "    consumer = pulcli.subscribe(\n",
    "                  topic='scraper-output',\n",
    "                  subscription_name=f\"pdf-process-consumer-{core}\",\n",
    "                  schema=JsonSchema(PDFInfo) )\n",
    "    while shutdown.empty():\n",
    "        try:\n",
    "            msg = consumer.receive_timeout(5000)\n",
    "            # Try to process message? Not totally sure why this is needed.\n",
    "            try:\n",
    "                pdf_json = msg.value()\n",
    "                print(f\"Processing message: {msg.message_id()}, PDF url: {pdf_json.url}\")\n",
    "                # Acknowledge successful processing of the message\n",
    "                consumer.acknowledge(msg)\n",
    "                with con.cursor() as cur:\n",
    "                    try:\n",
    "                        key = hashlib.md5(pdf_json.url.encode()).hexdigest()\n",
    "                        with BytesIO() as stream_buffer:\n",
    "                            response = requests.get(pdf_json.url, stream=True)\n",
    "                            if response.status_code != 200:\n",
    "                                raise Exception(f\"Failed to download PDF; received HTTP {response.status_code} from underlying server\")\n",
    "                            for chunk in response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
    "                                stream_buffer.write(chunk)\n",
    "                            doc = pymupdf.open(stream=stream_buffer, filetype=\"pdf\")\n",
    "                            cur.execute(sources_insert,(key,pdf_json.url,pdf_json.title,pdf_json.authors,pdf_json.summary,))\n",
    "                            for page in doc:\n",
    "                                try:\n",
    "                                    cur.execute(keyword_insert,(key,page.number,page.get_text().replace('\\x00', ''),))\n",
    "                                    chunks = text_splitter.split_text(page.get_text())\n",
    "                                    embeddings = embed.embed_documents(chunks)\n",
    "                                    for chunk_id, (chunk,embedding) in enumerate(zip(chunks,embeddings)):\n",
    "                                        cur.execute(semantic_insert,(key,page.number,chunk_id,embedding,))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing page {page.number} of {url}: {e}\")\n",
    "                                    continue  # Skip to the next page\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error downloading PDF from url: {url}: {e}\")\n",
    "                        continue  # Skip to the next page\n",
    "            except Exception:\n",
    "                # Message failed to be processed\n",
    "                consumer.negative_acknowledge(msg)\n",
    "        except Exception:\n",
    "            # We want timeout so we don't get stuck infinitely waiting for new message, need to evaluate shutdown variable every so often\n",
    "            continue\n",
    "    consumer.close()\n",
    "    con.close()\n",
    "        \n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Create a connection pool\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=cores,  # Minimum connections\n",
    "    maxconn=cores, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "join_list = []\n",
    "for core in range(0,cores):\n",
    "    worker = Thread(target=thread_task, args=(shutdown,pg_pool.getconn(),core))\n",
    "    worker.start()\n",
    "    join_list.append(worker)\n",
    "\n",
    "print(f\"Running {cores} workers\") \n",
    "        \n",
    "for thread in join_list:\n",
    "    thread.join()\n",
    "\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3460021-d0aa-43b3-91bd-2b64ad8dc975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
