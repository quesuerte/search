{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12911024-f7ea-4559-9d83-9ea7520f47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in /opt/conda/lib/python3.11/site-packages (0.3.6)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (0.3.20)\n",
      "Requirement already satisfied: pymupdf in /opt/conda/lib/python3.11/site-packages (1.25.3)\n",
      "Requirement already satisfied: langchain_ollama in /opt/conda/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.10)\n",
      "Requirement already satisfied: pgvector in /opt/conda/lib/python3.11/site-packages (0.3.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (5.3.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /opt/conda/lib/python3.11/site-packages (from langchain-text-splitters) (0.3.43)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.3.13)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/conda/lib/python3.11/site-packages (from langchain_ollama) (0.4.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/conda/lib/python3.11/site-packages (from selenium) (4.12.2)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (2.4)\n",
      "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m452.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m495.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, websocket-client, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.6.4\n",
      "    Uninstalling websocket-client-1.6.4:\n",
      "      Successfully uninstalled websocket-client-1.6.4\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.1.0 outcome-1.3.0.post0 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-text-splitters langchain pymupdf langchain_ollama psycopg2-binary pgvector numpy lxml selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c304475f-c0e6-41b1-a640-31db83e389f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,  \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    ")\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"all-minilm\",\n",
    "    base_url = \"http://host.docker.internal:11434\"\n",
    ")\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=cores,  # Minimum connections\n",
    "    maxconn=cores, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "\n",
    "# Load the PDF\n",
    "url = \"https://assets.openstax.org/oscms-prodcms/media/documents/Calculus_Volume_1_-_WEB_68M1Z5W.pdf\"  # PDF path or URL\n",
    "\n",
    "stream = BytesIO(urllib.request.urlopen(url).read())\n",
    "doc = pymupdf.open(stream=stream, filetype=\"pdf\")\n",
    "print(len(doc))\n",
    "# for page in doc:\n",
    "#     chunks = text_splitter.split_text(page.get_text())\n",
    "#     embeddings = embed.embed_documents(chunks)\n",
    "#     print(len(chunks))\n",
    "#     print(len(embeddings))\n",
    "#     for chunk_id, (chunk, embedding) in enumerate(zip(chunks,embeddings)):\n",
    "#         print(f\"Page: {page.number}, Chunk: {chunk_id}, Content: {chunk}\")#, Embedding: {embedding}\")\n",
    "#         print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb6ddf0-28fd-4b1f-9ad6-fccfbc505564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d3411b6a43b2a6300108a85a3270b468', 10, 'https://arxiv.org/pdf/2502.15279', None, None, 0.72788135575541)\n"
     ]
    }
   ],
   "source": [
    "from pgvector.psycopg2 import register_vector\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"all-minilm\",\n",
    "    base_url = \"http://host.docker.internal:11434\"\n",
    ")\n",
    "\n",
    "search = np.array(embed.embed_query('hello'))\n",
    "from psycopg2 import pool\n",
    "db = 'search'\n",
    "user = 'postgres'\n",
    "password = 'Washington17354!'\n",
    "host = '192.168.0.102'\n",
    "query = \"\"\"SELECT a.id, a.page, b.uri, b.title, b.author, a.embedding <=> %s AS rank\n",
    "FROM semantic_search a\n",
    "INNER JOIN sources b ON a.id = b.id\n",
    "ORDER BY rank\n",
    "LIMIT %s\"\"\"\n",
    "limit = 5\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=1,  # Minimum connections\n",
    "    maxconn=4, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "with pg_pool.getconn() as con:\n",
    "    register_vector(con)\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(query,(search,limit,));\n",
    "        result = cur.fetchone()\n",
    "        print(result);\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a172f01c-b00f-4dbb-899d-ced4863eb955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/abs/1704.08000', 'https://arxiv.org/abs/2012.10711', 'https://arxiv.org/abs/2101.11003', 'https://arxiv.org/abs/2104.11977', 'https://arxiv.org/abs/2106.05421', 'https://arxiv.org/abs/2204.08027', 'https://arxiv.org/abs/2209.12526', 'https://arxiv.org/abs/2210.01242', 'https://arxiv.org/abs/2210.16010', 'https://arxiv.org/abs/2212.11805']\n",
      "A Framework for Algorithm Stability\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(arxiv_page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle mathjax\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrim\u001b[49m(soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblockquote\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract mathjax\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAbstract:\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m([a\u001b[38;5;241m.\u001b[39mget_text() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m a\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthors\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownload-pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trim' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "# First we want to get all the identifiers we're going to process\n",
    "URL = \"http://export.arxiv.org/oai2?set=cs&verb=ListIdentifiers&metadataPrefix=oai_dc&from=2025-03-08\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "oai_soup = BeautifulSoup(page.content, \"xml\")\n",
    "ids = oai_soup.find_all(\"identifier\")\n",
    "texts = [id.get_text().replace('oai:arXiv.org:','https://arxiv.org/abs/') for id in ids]\n",
    "print(texts[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edc492d4-9829-44ca-a932-6e2f07491978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Framework for Algorithm Stability\n",
      "We say that an algorithm is stable if small changes in the input result in small changes in the output. This kind of algorithm stability is particularly relevant when analyzing and visualizing time-varying data. Stability in general plays an important role in a wide variety of areas, such as numerical analysis, machine learning, and topology, but is poorly understood in the context of (combinatorial) algorithms. In this paper we present a framework for analyzing the stability of algorithms. We focus in particular on the trade-off between the stability of an algorithm and the quality of the solution it computes. Our framework allows for three types of stability analysis with increasing degrees of complexity: event stability, topological stability, and Lipschitz stability. In addition, we need to refine the model of an algorithm based on how it interacts with the time-varying data, for which we consider several options. We demonstrate the use of our stability framework by applying it to kinetic Euclidean minimum spanning trees.\n",
      "Wouter Meulemans (https://arxiv.org/search/cs?searchtype=author&query=Meulemans,+W)\n",
      "Bettina Speckmann (https://arxiv.org/search/cs?searchtype=author&query=Speckmann,+B)\n",
      "Kevin Verbeek (https://arxiv.org/search/cs?searchtype=author&query=Verbeek,+K)\n",
      "Jules Wulms (https://arxiv.org/search/cs?searchtype=author&query=Wulms,+J)\n",
      "https://arxiv.org/pdf/1704.08000\n"
     ]
    }
   ],
   "source": [
    "for uri in texts[:1]:\n",
    "    time.sleep(3)\n",
    "    arxiv_page = requests.get(uri)\n",
    "    soup = BeautifulSoup(arxiv_page.content, \"html.parser\")\n",
    "    print(soup.find(\"h1\",class_='title mathjax').get_text().replace('Title:',''))\n",
    "    print(soup.find(\"blockquote\", class_='abstract mathjax').get_text().replace('Abstract:','').strip())\n",
    "    print('\\n'.join([a.get_text() + \" (\" + a.get('href') + \")\" for a in soup.find(\"div\",class_='authors').find_all('a')]))\n",
    "    print('https://arxiv.org' + soup.find('a',class_='download-pdf').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62cb68d3-d7ad-4127-9ae8-c127fdb83151",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service /home/jovyan/.cache/selenium/chromedriver/linux64/134.0.6998.35/chromedriver unexpectedly exited. Status code was: 127\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n\u001b[0;32m----> 6\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Or other browser driver\u001b[39;00m\n\u001b[1;32m      7\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://openstax.org/details/books/calculus-volume-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Wait for specific elements to load (e.g., using XPath)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/chrome/webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/chromium/webdriver.py:55\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m.\u001b[39mbrowser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39menv_path() \u001b[38;5;129;01mor\u001b[39;00m finder\u001b[38;5;241m.\u001b[39mget_driver_path()\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     59\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/common/service.py:113\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_process_still_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connectable():\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/selenium/webdriver/common/service.py:126\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unexpectedly exited. Status code was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service /home/jovyan/.cache/selenium/chromedriver/linux64/134.0.6998.35/chromedriver unexpectedly exited. Status code was: 127\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome()  # Or other browser driver\n",
    "driver.get('https://openstax.org/details/books/calculus-volume-1')\n",
    "\n",
    "try:\n",
    "    # Wait for specific elements to load (e.g., using XPath)\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@class='loc-summary-text']\"))\n",
    "    )\n",
    "    html_content = driver.page_source\n",
    "    print(html_content)\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "print(soup)\n",
    "print(soup.find(\"h1\",class_='image-heading'))#.find('img').get('alt'))\n",
    "print(soup.find(\"div\", class_='loc-summary-text').find('p').get_text())\n",
    "print('\\n'.join(soup.find_all('div', class_='loc-senior-author')) +'\\n' + '\\n'.join(soup.find_all('div', class_='loc-nonsenior-author')))\n",
    "print(soup.find('a',string='Download a PDF').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7afefb03-347d-40d6-844b-f2df0925607f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting insert of documents into Postgres\n",
      "Processing PDF: https://arxiv.org/abs/1704.08000\n",
      "Processing PDF: https://arxiv.org/abs/2012.10711\n",
      "Processing PDF: https://arxiv.org/abs/2101.11003\n",
      "Processing PDF: https://arxiv.org/abs/2104.11977\n",
      "Processing PDF: https://arxiv.org/abs/2106.05421\n",
      "Processing PDF: https://arxiv.org/abs/2204.08027\n",
      "Processing PDF: https://arxiv.org/abs/2209.12526\n",
      "Processing PDF: https://arxiv.org/abs/2210.01242\n",
      "Processing PDF: https://arxiv.org/abs/2210.16010\n",
      "Processing PDF: https://arxiv.org/abs/2212.11805\n",
      "Completed inserting documents in 40.0526180267334 seconds\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "import multiprocessing\n",
    "from threading import Thread\n",
    "from psycopg2 import pool\n",
    "import time\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "import io\n",
    "import random\n",
    "import requests\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "host = \"192.168.0.102\"\n",
    "user = \"postgres\"\n",
    "password = \"Washington17354!\"\n",
    "db = \"search\"\n",
    "oai_uri = \"http://export.arxiv.org/oai2?set=cs&verb=ListIdentifiers&metadataPrefix=oai_dc&from=2025-03-08\"\n",
    "page = requests.get(oai_uri)\n",
    "oai_soup = BeautifulSoup(page.content, \"xml\")\n",
    "ids = oai_soup.find_all(\"identifier\")\n",
    "urls = [id.get_text().replace('oai:arXiv.org:','https://arxiv.org/abs/') for id in ids][:10]\n",
    "\n",
    "\n",
    "sources_insert = \"INSERT INTO sources (id, uri, title, author, summary) VALUES (%s, %s, %s, %s, %s)\"\n",
    "#\"\"\"INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "#VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n",
    "semantic_insert = \"INSERT INTO semantic_search (id, page, chunk, embedding) VALUES (%s, %s, %s, %s)\"\n",
    "keyword_insert = \"INSERT INTO keyword_search (id, page, ts) VALUES (%s, %s, to_tsvector('english', %s))\"\n",
    "\n",
    "def process_resource(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    if (url.contains('arxiv')):\n",
    "        title = soup.find(\"h1\",class_='title mathjax').get_text().replace('Title:','')\n",
    "        summary = soup.find(\"blockquote\", class_='abstract mathjax').get_text().replace('Abstract:','').strip()\n",
    "        authors = '\\n'.join([a.get_text() + \" (\" + a.get('href') + \")\" for a in soup.find(\"div\",class_='authors').find_all('a')])\n",
    "        pdf_url = 'https://arxiv.org' + soup.find('a',class_='download-pdf').get('href')\n",
    "    else:\n",
    "        \n",
    "\n",
    "def thread_task(inq,con,core,chunk_size=500, chunk_overlap=50):\n",
    "    embed = OllamaEmbeddings(\n",
    "        model=\"all-minilm\",\n",
    "        base_url = \"http://host.docker.internal:11434\"\n",
    "    )\n",
    "    # LangChain's Recursive Text Splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    "    )\n",
    "    while True:\n",
    "        with con.cursor() as cur:\n",
    "            url = inq.get()\n",
    "            if url is None:\n",
    "                inq.task_done()\n",
    "                con_queue.put(con)\n",
    "                break\n",
    "            print(f\"Processing PDF: {url}\")\n",
    "            try:\n",
    "                key = hashlib.md5(url.encode()).hexdigest()\n",
    "                time.sleep(3)\n",
    "                (title,summary,authors,pdf_url) = process_resource(url)\n",
    "                with BytesIO() as stream_buffer:\n",
    "                    response = requests.get(pdf_url, stream=True)\n",
    "                    if response.status_code != 200:\n",
    "                        raise Exception(f\"Failed to download PDF; received HTTP {response.status_code} from underlying server\")\n",
    "                    for chunk in response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
    "                        stream_buffer.write(chunk)\n",
    "                    doc = pymupdf.open(stream=stream_buffer, filetype=\"pdf\")\n",
    "                    cur.execute(sources_insert,(key,pdf_url,title,authors,summary,))\n",
    "                    for page in doc:\n",
    "                        try:\n",
    "                            cur.execute(keyword_insert,(key,page.number,page.get_text().replace('\\x00', ''),))\n",
    "                            chunks = text_splitter.split_text(page.get_text())\n",
    "                            embeddings = embed.embed_documents(chunks)\n",
    "                            for chunk_id, (chunk,embedding) in enumerate(zip(chunks,embeddings)):\n",
    "                                cur.execute(semantic_insert,(key,page.number,chunk_id,embedding,))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing page {page.number} of {url}: {e}\")\n",
    "                            continue  # Skip to the next page\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading PDF from url: {url}: {e}\")\n",
    "                continue  # Skip to the next page\n",
    "\n",
    "            con.commit()\n",
    "            inq.task_done()\n",
    "            time.sleep(3)\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "insert_queue = Queue()\n",
    "\n",
    "for url in urls:\n",
    "    insert_queue.put(url)\n",
    "# Create a connection pool\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=cores,  # Minimum connections\n",
    "    maxconn=cores, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "con_queue = Queue()\n",
    "for core in range(0,cores):\n",
    "    insert_queue.put(None)\n",
    "    con_queue.put(None)\n",
    "\n",
    "print(\"Starting insert of documents into Postgres\")\n",
    "insert_start_time = time.time()\n",
    "join_list = []\n",
    "for core in range(0,cores):\n",
    "    worker = Thread(target=thread_task, args=(insert_queue,pg_pool.getconn(),core))\n",
    "    worker.start()\n",
    "    join_list.append(worker)\n",
    "    time.sleep(3)\n",
    "        \n",
    "for thread in join_list:\n",
    "    thread.join()\n",
    "\n",
    "while not con_queue.empty():\n",
    "    connection = con_queue.get()\n",
    "    if connection is None:\n",
    "        con_queue.task_done()\n",
    "        break\n",
    "    connection.close()\n",
    "    con_queue.task_done()\n",
    "\n",
    "insert_end_time = time.time()\n",
    "print(f\"Completed inserting documents in {insert_end_time - insert_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4473874-d665-4fa1-8c40-ffc75fa17079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=mydb user=myuser password=mypass host=localhost\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\", (doc_id, \"path/to/file.pdf\", title, summary, title_embedding, summary_embedding))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8afa7f-1808-4ee8-a4dc-e5a5580acec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"FRANK ELAVSKY, Carnegie Mellon University, fje@cmu.edu\\nAcademic, peer-reviewed 'short' papers are a common way to present a late-breaking work to the academic community that outlines preliminary findings, research ideas, and novel conversations. By comparison, blogging or writing posts on social media are an unstructured and open way to discuss ideas and start new conversations. Both have limitations in the proliferation of research ideas. The short paper format relies on the conference and journal submission process while blogging does not operate within a structured format or set of expectations at all. However, at times the demand exists for late-breaking ideas and conversations to arise in a raw form or with urgency but should still be archived and recorded in a way that promotes citational honesty and integrity. To address this, I present: The Micro-Paper, as a micro-paper itself. The Micro-Paper is a small, cheap, accessible, digital document that is self-published and archived, akin to a pre-print of a short paper. This meta micro-paper discusses the context, goals, and considerations of micro-paper authoring.\\nFig 1: The micro-paper fills a gap on the spectrum between fast, cheap ideas and rigorous, archival work.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=660.5159912109375, r=386.1499938964844, b=650.1840209960938, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 54))]), DocItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=635.8800048828125, r=541.1599731445312, b=523.75, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1079))]), DocItem(self_ref='#/texts/3', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=412.30999755859375, r=291.8800048828125, b=392.20001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 105))])], headings=['The Micro-Paper: Towards cheaper, citable research ideas and conversations'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='A micro-paper is a paper between 1 and 4 pages in length that engages a single idea clearly. A micro-paper can be anything from focused blog post to a preprinted short paper, but it is published through an open archive.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/7', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=363.5299987792969, r=297.6300048828125, b=320.4200134277344, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 219))])], headings=['1. What is a micro-paper?'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Ideas and conversations often arise in research settings in response to problems, gaps, or issues. But some ideas and conversations also come about in a more generative fashion: they are still responding to something within a context but are not concerned with problems and gaps. So, whether filling gaps or otherwise, a micro-paper (and even the authors at times) must clearly and reflexively be situated within a conversational context.\\nAt a meta-level, this paper is framed as a gapfiller (see: Fig 1). Currently, academic publishing is expensive, time-consuming, and high risk. And while much of the process could be argued as a necessary set of procedures to ensure we aren't making claims that are misleading or unfounded, there exists a gap for disseminating ideas that should be cheap, accessible, and intended to inspire other work [4]. Not all research conversation needs to have answers. The heart of research as a community is due to the dissemination of\\nideas that aren't only congealed or refined, but raw and messy as well.\\nGenerally, the gap for disseminating cheap ideas is filled by academics today through Twitter, blogging, or in some form of social media or another. Sometimes the gap is filled through workshops, position papers, or conversation pieces (such as ACM's Interactions). There are many options for sharing ideas, all with different tradeoffs between editorial and authoring expenses, time, archiving, accessibility, and democratization of the process.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/9', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=291.7799987792969, r=299.1050109863281, b=202.63999938964844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 438))]), DocItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=199.74000549316406, r=294.5299987792969, b=87.59500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 527))]), DocItem(self_ref='#/texts/11', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=509.3299865722656, r=538.8599853515625, b=489.2200012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 71))]), DocItem(self_ref='#/texts/12', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=486.3299865722656, r=540.4500122070312, b=385.70001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 446))])], headings=['2. The context of micro-papers'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"The micro-paper's focus is on ideas for the sake of generative work, conversation, and inspiration. In contrast, a micro-paper is not an appropriate venue for sharing findings, claims, or experiments. The nature of methodological generation of knowledge is most trustworthy when there is a more rigorous process in place. Some avenues generate good or trustworthy knowledge and ideas, but the micro-paper is a place for sharing potentially useful ideas. Good or trustworthy knowledge may require more careful review [3, 4, 6], but potentially useful ideas should at least be archived.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/14', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=343.2900085449219, r=539.3800048828125, b=219.64999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 584))])], headings=[\"3. A micro-paper's goal is the free, cheap, open, and honest dissemination of ideas\"], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='Whether having peer-reviewed work or an editorial team, most writing is costly in both time and money. It is also risky: your work could get rejected or require slow iterations of feedback and review.\\nIn contrast, this micro-paper took me 3 hours on a random Thursday in February (when I should be crunching for another deadline). Most micro-papers should ideally be short enough in length to encourage both rapid authoring and reading.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/16', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=191.97999572753906, r=537.25, b=148.8699951171875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 200))]), DocItem(self_ref='#/texts/17', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=145.97999572753906, r=534.219970703125, b=91.34500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 235))])], headings=['3.1 A micro-paper must be small and cheap'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"While blogging and tweeting is cheap and fast and encourages ideas to be shared, these aren't trustworthy archives. And sometimes good ideas arise in these faster, cheaper contexts that should be captured, articulated, and stored for later reference. With the existential threat of twitter disintegrating at any moment and the entirely unmaintained space of many academic blogs, it is important for some ideas to be archived [5].\\nWhile some blog maintainers may have higher standards for their longevity, url stability, dois, and records of changes, it may make sense for most to use an existing pre-print archival platform, like arXiv for ensuring trustworthiness and reliability in your readers.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/19', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=704.9000244140625, r=293.1050109863281, b=604.27001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 429))]), DocItem(self_ref='#/texts/20', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=601.3499755859375, r=297.17999267578125, b=546.739990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 267))])], headings=['3.2 A micro-paper must be archived'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='In addition to archival processes for the sake of accessing later, many archival sites (such as arXiv) also keep track of revision histories. The ephemeral and non-standardized way that individuals operate their own blogs and social media means that not only might something move or cease to exist (a findability problem) but there is also an honesty problem when contents change or update without record [7].' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/22', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=519.3300170898438, r=299.3999938964844, b=430.20001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 409))])], headings=['3.3 Revisions must also be archived'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Blogs, twitter, and mastodon have done more for disability discourse than most other media, but especially more than the dreaded PDF favored by academics. This is because text-based media online (generally in HTML or Markdown) has immense accessibility potential over PDF [1] (and paper publications). All artifacts of academic discourse, including every paper publication, should be more accessible. Due to a micro-paper's size, it is easier for authors to learn accessibility for than a full-size paper with proprietary formatting involved. This micro-paper was authored in Microsoft Word, exported as a PDF, and then converted into HTML using pandoc.\\nWith the push for arXiv to transition more towards accessible formats of publication [2], I believe that trustworthy archives that are accessibility-first are near. Micro-papers will compliment this push.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/24', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=402.55999755859375, r=295.6099853515625, b=255.89999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 653))]), DocItem(self_ref='#/texts/25', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=253.00999450683594, r=294.57000732421875, b=209.88999938964844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 204))])], headings=['3.4 A micro-paper must be accessible'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"If someone has an idea, conversation, or late-breaking work, they might consider the following questions: Why wouldn't I write a short academic paper and submit to a traditional venue?\\nIt is time-consuming, expensive, and requires waiting for the publication cycle. It is also higher risk, in cases where the peer review process might reject it.\\nWhy shouldn't I write a piece in a non-peer reviewed publication, like ACM Interactions?\\nThis is also time-consuming and higher risk, because editorial interest may conflict. In addition, these often require an existing network of colleagues, invitation to contribute, or formal submission and selection process.\\nWhy shouldn't I write a blog or twitter thread? Blogs and social media posts raise concerns about archival quality and trustworthiness. Accessing the piece later may become difficult or cumbersome. Some great ideas and discussions have been lost in time due to the ephemeral nature of these cheap and fast options.\\nWhen is writing a micro-paper a good idea?\\nNotably, there is no formal peer review for a micropaper. Our currently imagined peer review process may not make sense for all work published with the intent to push new ideas and conversations. It is even worth considering if this practice should continue at all [3, 4].\\nA practical use for a micro-paper may be as a preprint or early draft for an eventual short or full paper submission, position paper, or book chapter. The greatest strength of both the pre-print and short paper process is that they can invigorate scholars with new or raw ideas to see those turn into full projects. Short papers also have a core readership and opportunities to present that are not afforded to micro-papers, so for early career researchers it may be important to consider ways to use these two formats together.\\nFor folks who simply want to get a citable idea out into the world without regard for submission and publication cycles and procedures, a micro-paper is a good choice as well.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/27', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=167.47999572753906, r=291.0799865722656, b=124.36799621582031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 184))]), DocItem(self_ref='#/texts/28', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=121.44999694824219, r=292.875, b=89.84500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 160))]), DocItem(self_ref='#/texts/29', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=717.6599731445312, r=530.8699951171875, b=697.5380249023438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 88))]), DocItem(self_ref='#/texts/30', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=694.6500244140625, r=538.22998046875, b=651.52001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 223))]), DocItem(self_ref='#/texts/31', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=637.1400146484375, r=539.969970703125, b=570.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 314))]), DocItem(self_ref='#/texts/32', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=556.6099853515625, r=495.70001220703125, b=547.9879760742188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 42))]), DocItem(self_ref='#/texts/33', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=545.0999755859375, r=540.5700073242188, b=490.4700012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 272))]), DocItem(self_ref='#/texts/34', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=487.5799865722656, r=540.7000122070312, b=375.45001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 528))]), DocItem(self_ref='#/texts/35', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=372.55999755859375, r=541.969970703125, b=329.4200134277344, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 175))])], headings=['4. Considering when a micro-paper is the write choice (pun intended)'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"And lastly, there may be authors with too many ideas to pursue (even when some are useful) and they are willing to admit that they won't pursue every idea that they have. A micro-paper is a way to put the idea into the discussion and let it run its course. In my case, I recognize that some problems and patterns are outside of the scope of my leverage and experience to address, such as contributions to design or behavioral domains of accessibility (when my area is strictly technical contributions).\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/36', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=326.5299987792969, r=536.969970703125, b=214.39999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 502))])], headings=['4. Considering when a micro-paper is the write choice (pun intended)'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='The hope is that both the procedural and systemic inaccessibility of the short paper authoring process and the citational uncertainty of blogs and social media can be addressed with the micro-paper. I hope to see early, usable ideas shared more freely and especially hope to invigorate young scholars and include historically excluded folks, such as those with disabilities, in the larger research conversation.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/38', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=185.72999572753906, r=538.219970703125, b=96.59500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 411))])], headings=['5. Conclusion'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Special thanks to Jonathan Zong, for your encouragement to make my cheap ideas citable and for feedback on this micro-paper. Also thanks to the folks at the MIT Vis Lab (Arvind, Crystal, and Alan) for supporting my nottraditionally-publishable ideas over the past couple years especially. Hearing that my 'tweets are a public service' encouraged me to make the heart of that service last longer than Twitter does (hopefully).\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/40', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=691.9000244140625, r=532.3699951171875, b=648.77001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 425))])], headings=['Acknowledgements'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"[1] Bigham, Jeffrey et al. 'An Uninteresting Tour Through Why Our Research Papers Aren't Accessible.' CHI , May 2016. doi: 10.1145/2851581.2892588.\\n[2] Brinn, Shamsi et al. 'A framework for improving the accessibility of research papers on arXiv.org.' arXiv , Dec. 2022. doi: 10.48550/ARXIV.2212.07286\\nAccessed: Feb. 24, 2023.\\n[3] Mastroianni, Adam. 'The rise and fall of peer review.' Experimental History, https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review\\n[4] Mastroianni, Adam et al. 'Things Could Be Better.' PsyArXiv , Nov 2022.\\n[5] Sanderson, Robert et al. 'Analyzing the Persistence of Referenced Web Resources with Memento.' arXiv , May 2011. doi: 10.48550/arXiv.1105.3459\\n[6] 'Why Do People Publish on Arxiv Instead of Other Places?' Academia Stack Exchange , Accessed: Feb. 23, 2023, https://academia.stackexchange.com/questions/75325/why-do-people-publish-on-arxiv-instead-ofother-places.\\n[7] Zittrain, Jonathan et al. 'Perma: Scoping and Addressing the Problem of Link and Reference Rot in Legal Citations.' Harvard Law Review , Mar. 2014.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/42', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=620.1400146484375, r=541.7000122070312, b=600.02001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 147))]), DocItem(self_ref='#/texts/43', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=597.1099853515625, r=537.969970703125, b=576.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 153))]), DocItem(self_ref='#/texts/44', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=397.4200134277344, t=574.0999755859375, r=502.8599853515625, b=565.489990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 24))]), DocItem(self_ref='#/texts/45', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=574.1099853515625, r=407.17999267578125, b=553.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 156))]), DocItem(self_ref='#/texts/46', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=551.1099853515625, r=387.6499938964844, b=542.4879760742188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 75))]), DocItem(self_ref='#/texts/47', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=539.6099853515625, r=535.719970703125, b=519.489990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 146))]), DocItem(self_ref='#/texts/48', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=516.5900268554688, r=521.7000122070312, b=484.9700012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 218))]), DocItem(self_ref='#/texts/49', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=482.0799865722656, r=512.4099731445312, b=461.9679870605469, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 151))])], headings=['References'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n"
     ]
    }
   ],
   "source": [
    "# for page_num, page in enumerate(doc.pages):\n",
    "#     chunks = page.chunk()  # Split into smaller parts\n",
    "\n",
    "#     for chunk_idx, chunk in enumerate(chunks):\n",
    "#         embedding = doc.embed(chunk.text)\n",
    "\n",
    "#         # Insert into semantic_search\n",
    "#         cur.execute(\"\"\"\n",
    "#             INSERT INTO semantic_search (id, page, chunk, embedding)\n",
    "#             VALUES (%s, %s, %s, %s)\n",
    "#         \"\"\", (doc_id, page_num + 1, chunk_idx, embedding))\n",
    "\n",
    "#         # Insert into keyword_search\n",
    "#         cur.execute(\"\"\"\n",
    "#             INSERT INTO keyword_search (id, page, ts)\n",
    "#             VALUES (%s, %s, to_tsvector('english', %s))\n",
    "#         \"\"\", (doc_id, page_num + 1, chunk.text))\n",
    "\n",
    "# conn.commit()\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ab7aa-8e54-4cb7-9be0-5946932ef720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
