{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12911024-f7ea-4559-9d83-9ea7520f47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-text-splitters\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pymupdf\n",
      "  Using cached pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting langchain_ollama\n",
      "  Using cached langchain_ollama-0.2.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-text-splitters)\n",
      "  Using cached langchain_core-0.3.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.11-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting numpy<2,>=1.26.4 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting ollama<1,>=0.4.4 (from langchain_ollama)\n",
      "  Using cached ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (4.8.0)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.34->langchain-text-splitters)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.0.0)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Using cached langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "Using cached pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "Using cached langchain_ollama-0.2.3-py3-none-any.whl (19 kB)\n",
      "Using cached psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached langchain_core-0.3.39-py3-none-any.whl (414 kB)\n",
      "Using cached langsmith-0.3.11-py3-none-any.whl (335 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Using cached orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Using cached propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Using cached zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: zstandard, typing-extensions, tenacity, pymupdf, psycopg2-binary, propcache, orjson, numpy, multidict, h11, frozenlist, annotated-types, aiohappyeyeballs, yarl, requests-toolbelt, pydantic-core, httpcore, aiosignal, pydantic, httpx, aiohttp, ollama, langsmith, langchain-core, langchain-text-splitters, langchain_ollama, langchain\n",
      "  Attempting uninstall: zstandard\n",
      "    Found existing installation: zstandard 0.21.0\n",
      "    Uninstalling zstandard-0.21.0:\n",
      "      Successfully uninstalled zstandard-0.21.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 annotated-types-0.7.0 frozenlist-1.5.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 langchain-0.3.19 langchain-core-0.3.39 langchain-text-splitters-0.3.6 langchain_ollama-0.2.3 langsmith-0.3.11 multidict-6.1.0 numpy-1.26.4 ollama-0.4.7 orjson-3.10.15 propcache-0.3.0 psycopg2-binary-2.9.10 pydantic-2.10.6 pydantic-core-2.27.2 pymupdf-1.25.3 requests-toolbelt-1.0.0 tenacity-9.0.0 typing-extensions-4.12.2 yarl-1.18.3 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-text-splitters langchain pymupdf langchain_ollama psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304475f-c0e6-41b1-a640-31db83e389f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,  \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    ")\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"all-minilm\",\n",
    "    base_url = \"http://host.docker.internal:11434\"\n",
    ")\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=cores,  # Minimum connections\n",
    "    maxconn=cores, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "\n",
    "# Load the PDF\n",
    "url = \"https://arxiv.org/pdf/2302.12854\"  # PDF path or URL\n",
    "\n",
    "stream = BytesIO(urllib.request.urlopen(url).read())\n",
    "doc = pymupdf.open(stream=stream, filetype=\"pdf\")\n",
    "for page in doc:\n",
    "    chunks = text_splitter.split_text(page.get_text())\n",
    "    embeddings = embed.embed_documents(chunks)\n",
    "    print(len(chunks))\n",
    "    print(len(embeddings))\n",
    "    for chunk_id, (chunk, embedding) in enumerate(zip(chunks,embeddings)):\n",
    "        print(f\"Page: {page.number}, Chunk: {chunk_id}, Content: {chunk}, Embedding: {embedding}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afefb03-347d-40d6-844b-f2df0925607f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting insert of documents into Postgres\n",
      "Processing PDF: https://arxiv.org/pdf/2502.14927\n",
      "Processing PDF: https://arxiv.org/pdf/2502.14980\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15150\n",
      "Error downloading PDF from url: https://arxiv.org/pdf/2502.14980: ('Connection broken: IncompleteRead(2097152 bytes read, 1253046 more expected)', IncompleteRead(2097152 bytes read, 1253046 more expected))\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15268\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15272\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15337\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15388\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15460\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15510\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15658\n",
      "Processing PDF: https://arxiv.org/pdf/2502.14953\n",
      "Processing PDF: https://arxiv.org/pdf/2502.14955\n",
      "Processing PDF: https://arxiv.org/pdf/2502.14962\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15108\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15279\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15541\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15593\n",
      "Processing PDF: https://arxiv.org/pdf/2502.15627\n",
      "Processing PDF: https://arxiv.org/pdf/2404.15977\n",
      "Processing PDF: https://arxiv.org/pdf/2409.09802\n",
      "Processing PDF: https://arxiv.org/pdf/2409.16328\n",
      "Processing PDF: https://arxiv.org/pdf/2410.00264\n",
      "Processing PDF: https://arxiv.org/pdf/2412.18651\n",
      "Processing PDF: https://arxiv.org/pdf/2412.20983\n",
      "Processing PDF: https://arxiv.org/pdf/2501.18144\n",
      "Processing PDF: https://arxiv.org/pdf/2502.05644\n",
      "Error downloading PDF from url: https://arxiv.org/pdf/2502.05644: Failed to open stream\n",
      "Processing PDF: https://arxiv.org/pdf/2502.09210\n",
      "Processing PDF: https://arxiv.org/pdf/2301.04677\n",
      "Processing PDF: https://arxiv.org/pdf/2305.00222\n",
      "Processing PDF: https://arxiv.org/pdf/2308.16076\n",
      "Processing PDF: https://arxiv.org/pdf/2403.04364\n",
      "Processing PDF: https://arxiv.org/pdf/2403.13484\n",
      "Processing PDF: https://arxiv.org/pdf/2403.18772\n",
      "Processing PDF: https://arxiv.org/pdf/2403.19520\n",
      "Processing PDF: https://arxiv.org/pdf/2405.01887\n",
      "Processing PDF: https://arxiv.org/pdf/2406.00207\n",
      "Processing PDF: https://arxiv.org/pdf/2412.17898\n",
      "Processing PDF: https://arxiv.org/pdf/2501.18730\n",
      "Completed inserting documents in 102.03515219688416 seconds\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "import multiprocessing\n",
    "from threading import Thread\n",
    "from psycopg2 import pool\n",
    "import time\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "import io\n",
    "import random\n",
    "import requests\n",
    "import pymupdf  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "host = \"pdf-service\"\n",
    "user = \"postgres\"\n",
    "password = \"admin\"\n",
    "db = \"postgres\"\n",
    "#urls = [\"https://arxiv.org/pdf/2302.12854\"] # PDF path or URL\n",
    "urls = [\"https://arxiv.org/pdf/2502.14927\", \"https://arxiv.org/pdf/2502.14980\", \"https://arxiv.org/pdf/2502.15150\", \"https://arxiv.org/pdf/2502.15268\", \"https://arxiv.org/pdf/2502.15272\", \"https://arxiv.org/pdf/2502.15337\", \"https://arxiv.org/pdf/2502.15388\", \"https://arxiv.org/pdf/2502.15460\", \"https://arxiv.org/pdf/2502.15510\", \"https://arxiv.org/pdf/2502.15658\", \"https://arxiv.org/pdf/2502.14953\", \"https://arxiv.org/pdf/2502.14955\", \"https://arxiv.org/pdf/2502.14962\", \"https://arxiv.org/pdf/2502.15108\", \"https://arxiv.org/pdf/2502.15279\", \"https://arxiv.org/pdf/2502.15541\", \"https://arxiv.org/pdf/2502.15593\", \"https://arxiv.org/pdf/2502.15627\", \"https://arxiv.org/pdf/2404.15977\", \"https://arxiv.org/pdf/2409.09802\", \"https://arxiv.org/pdf/2409.16328\", \"https://arxiv.org/pdf/2410.00264\", \"https://arxiv.org/pdf/2412.18651\", \"https://arxiv.org/pdf/2412.20983\", \"https://arxiv.org/pdf/2501.18144\", \"https://arxiv.org/pdf/2502.05644\", \"https://arxiv.org/pdf/2502.09210\", \"https://arxiv.org/pdf/2301.04677\", \"https://arxiv.org/pdf/2305.00222\", \"https://arxiv.org/pdf/2308.16076\", \"https://arxiv.org/pdf/2403.04364\", \"https://arxiv.org/pdf/2403.13484\", \"https://arxiv.org/pdf/2403.18772\", \"https://arxiv.org/pdf/2403.19520\", \"https://arxiv.org/pdf/2405.01887\", \"https://arxiv.org/pdf/2406.00207\", \"https://arxiv.org/pdf/2412.17898\", \"https://arxiv.org/pdf/2501.18730\"]\n",
    "\n",
    "sources_insert = \"INSERT INTO sources (id, uri) VALUES (%s, %s)\"\n",
    "#\"\"\"INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "#VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n",
    "semantic_insert = \"INSERT INTO semantic_search (id, page, chunk, embedding) VALUES (%s, %s, %s, %s)\"\n",
    "keyword_insert = \"INSERT INTO keyword_search (id, page, ts) VALUES (%s, %s, to_tsvector('english', %s))\"\n",
    "\n",
    "def thread_task(inq,con,core,chunk_size=500, chunk_overlap=50):\n",
    "    embed = OllamaEmbeddings(\n",
    "        model=\"all-minilm\",\n",
    "        base_url = \"http://host.docker.internal:11434\"\n",
    "    )\n",
    "    # LangChain's Recursive Text Splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,  \n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Prioritize logical breaks\n",
    "    )\n",
    "    while True:\n",
    "        with con.cursor() as cur:\n",
    "            time.sleep(random.random()*(cores/2))\n",
    "            url = inq.get()\n",
    "            if url is None:\n",
    "                inq.task_done()\n",
    "                con_queue.put(con)\n",
    "                break\n",
    "            print(f\"Processing PDF: {url}\")\n",
    "            try:\n",
    "                key = hashlib.md5(url.encode()).hexdigest()\n",
    "                with BytesIO() as stream_buffer:\n",
    "                    response = requests.get(url, stream=True)\n",
    "                    if response.status_code != 200:\n",
    "                        raise Exception(f\"Failed to download PDF; received HTTP {response.status_code} from underlying server\")\n",
    "                    for chunk in response.iter_content(chunk_size=io.DEFAULT_BUFFER_SIZE):\n",
    "                        stream_buffer.write(chunk)\n",
    "                    doc = pymupdf.open(stream=stream_buffer, filetype=\"pdf\")\n",
    "                    cur.execute(sources_insert,(key,url,))\n",
    "                    for page in doc:\n",
    "                        try:\n",
    "                            cur.execute(keyword_insert,(key,page.number,page.get_text().replace('\\x00', ''),))\n",
    "                            chunks = text_splitter.split_text(page.get_text())\n",
    "                            embeddings = embed.embed_documents(chunks)\n",
    "                            for chunk_id, (chunk,embedding) in enumerate(zip(chunks,embeddings)):\n",
    "                                cur.execute(semantic_insert,(key,page.number,chunk_id,embedding,))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing page {page.number} of {url}: {e}\")\n",
    "                            continue  # Skip to the next page\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading PDF from url: {url}: {e}\")\n",
    "                continue  # Skip to the next page\n",
    "\n",
    "            con.commit()\n",
    "            inq.task_done()\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "insert_queue = Queue()\n",
    "\n",
    "for url in urls:\n",
    "    insert_queue.put(url)\n",
    "# Create a connection pool\n",
    "pg_pool = pool.SimpleConnectionPool(\n",
    "    minconn=cores,  # Minimum connections\n",
    "    maxconn=cores, # Maximum connections\n",
    "    dsn=f\"dbname={db} user={user} password={password} host={host}\"\n",
    ")\n",
    "con_queue = Queue()\n",
    "for core in range(0,cores):\n",
    "    insert_queue.put(None)\n",
    "    con_queue.put(None)\n",
    "\n",
    "print(\"Starting insert of documents into Postgres\")\n",
    "insert_start_time = time.time()\n",
    "join_list = []\n",
    "for core in range(0,cores):\n",
    "    worker = Thread(target=thread_task, args=(insert_queue,pg_pool.getconn(),core))\n",
    "    worker.start()\n",
    "    join_list.append(worker)\n",
    "        \n",
    "for thread in join_list:\n",
    "    thread.join()\n",
    "\n",
    "while not con_queue.empty():\n",
    "    connection = con_queue.get()\n",
    "    if connection is None:\n",
    "        con_queue.task_done()\n",
    "        break\n",
    "    connection.close()\n",
    "    con_queue.task_done()\n",
    "\n",
    "insert_end_time = time.time()\n",
    "print(f\"Completed inserting documents in {insert_end_time - insert_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4473874-d665-4fa1-8c40-ffc75fa17079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=mydb user=myuser password=mypass host=localhost\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    INSERT INTO sources (id, uri, title, summary, title_embedding, summary_embedding)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\", (doc_id, \"path/to/file.pdf\", title, summary, title_embedding, summary_embedding))\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8afa7f-1808-4ee8-a4dc-e5a5580acec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"FRANK ELAVSKY, Carnegie Mellon University, fje@cmu.edu\\nAcademic, peer-reviewed 'short' papers are a common way to present a late-breaking work to the academic community that outlines preliminary findings, research ideas, and novel conversations. By comparison, blogging or writing posts on social media are an unstructured and open way to discuss ideas and start new conversations. Both have limitations in the proliferation of research ideas. The short paper format relies on the conference and journal submission process while blogging does not operate within a structured format or set of expectations at all. However, at times the demand exists for late-breaking ideas and conversations to arise in a raw form or with urgency but should still be archived and recorded in a way that promotes citational honesty and integrity. To address this, I present: The Micro-Paper, as a micro-paper itself. The Micro-Paper is a small, cheap, accessible, digital document that is self-published and archived, akin to a pre-print of a short paper. This meta micro-paper discusses the context, goals, and considerations of micro-paper authoring.\\nFig 1: The micro-paper fills a gap on the spectrum between fast, cheap ideas and rigorous, archival work.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/1', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=660.5159912109375, r=386.1499938964844, b=650.1840209960938, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 54))]), DocItem(self_ref='#/texts/2', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=635.8800048828125, r=541.1599731445312, b=523.75, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 1079))]), DocItem(self_ref='#/texts/3', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.CAPTION: 'caption'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=412.30999755859375, r=291.8800048828125, b=392.20001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 105))])], headings=['The Micro-Paper: Towards cheaper, citable research ideas and conversations'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='A micro-paper is a paper between 1 and 4 pages in length that engages a single idea clearly. A micro-paper can be anything from focused blog post to a preprinted short paper, but it is published through an open archive.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/7', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=363.5299987792969, r=297.6300048828125, b=320.4200134277344, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 219))])], headings=['1. What is a micro-paper?'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Ideas and conversations often arise in research settings in response to problems, gaps, or issues. But some ideas and conversations also come about in a more generative fashion: they are still responding to something within a context but are not concerned with problems and gaps. So, whether filling gaps or otherwise, a micro-paper (and even the authors at times) must clearly and reflexively be situated within a conversational context.\\nAt a meta-level, this paper is framed as a gapfiller (see: Fig 1). Currently, academic publishing is expensive, time-consuming, and high risk. And while much of the process could be argued as a necessary set of procedures to ensure we aren't making claims that are misleading or unfounded, there exists a gap for disseminating ideas that should be cheap, accessible, and intended to inspire other work [4]. Not all research conversation needs to have answers. The heart of research as a community is due to the dissemination of\\nideas that aren't only congealed or refined, but raw and messy as well.\\nGenerally, the gap for disseminating cheap ideas is filled by academics today through Twitter, blogging, or in some form of social media or another. Sometimes the gap is filled through workshops, position papers, or conversation pieces (such as ACM's Interactions). There are many options for sharing ideas, all with different tradeoffs between editorial and authoring expenses, time, archiving, accessibility, and democratization of the process.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/9', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=291.7799987792969, r=299.1050109863281, b=202.63999938964844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 438))]), DocItem(self_ref='#/texts/10', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=72.0250015258789, t=199.74000549316406, r=294.5299987792969, b=87.59500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 527))]), DocItem(self_ref='#/texts/11', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=509.3299865722656, r=538.8599853515625, b=489.2200012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 71))]), DocItem(self_ref='#/texts/12', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=486.3299865722656, r=540.4500122070312, b=385.70001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 446))])], headings=['2. The context of micro-papers'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"The micro-paper's focus is on ideas for the sake of generative work, conversation, and inspiration. In contrast, a micro-paper is not an appropriate venue for sharing findings, claims, or experiments. The nature of methodological generation of knowledge is most trustworthy when there is a more rigorous process in place. Some avenues generate good or trustworthy knowledge and ideas, but the micro-paper is a place for sharing potentially useful ideas. Good or trustworthy knowledge may require more careful review [3, 4, 6], but potentially useful ideas should at least be archived.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/14', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=343.2900085449219, r=539.3800048828125, b=219.64999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 584))])], headings=[\"3. A micro-paper's goal is the free, cheap, open, and honest dissemination of ideas\"], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='Whether having peer-reviewed work or an editorial team, most writing is costly in both time and money. It is also risky: your work could get rejected or require slow iterations of feedback and review.\\nIn contrast, this micro-paper took me 3 hours on a random Thursday in February (when I should be crunching for another deadline). Most micro-papers should ideally be short enough in length to encourage both rapid authoring and reading.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/16', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=191.97999572753906, r=537.25, b=148.8699951171875, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 200))]), DocItem(self_ref='#/texts/17', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=1, bbox=BoundingBox(l=315.1300048828125, t=145.97999572753906, r=534.219970703125, b=91.34500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 235))])], headings=['3.1 A micro-paper must be small and cheap'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"While blogging and tweeting is cheap and fast and encourages ideas to be shared, these aren't trustworthy archives. And sometimes good ideas arise in these faster, cheaper contexts that should be captured, articulated, and stored for later reference. With the existential threat of twitter disintegrating at any moment and the entirely unmaintained space of many academic blogs, it is important for some ideas to be archived [5].\\nWhile some blog maintainers may have higher standards for their longevity, url stability, dois, and records of changes, it may make sense for most to use an existing pre-print archival platform, like arXiv for ensuring trustworthiness and reliability in your readers.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/19', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=704.9000244140625, r=293.1050109863281, b=604.27001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 429))]), DocItem(self_ref='#/texts/20', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=601.3499755859375, r=297.17999267578125, b=546.739990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 267))])], headings=['3.2 A micro-paper must be archived'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='In addition to archival processes for the sake of accessing later, many archival sites (such as arXiv) also keep track of revision histories. The ephemeral and non-standardized way that individuals operate their own blogs and social media means that not only might something move or cease to exist (a findability problem) but there is also an honesty problem when contents change or update without record [7].' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/22', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=519.3300170898438, r=299.3999938964844, b=430.20001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 409))])], headings=['3.3 Revisions must also be archived'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Blogs, twitter, and mastodon have done more for disability discourse than most other media, but especially more than the dreaded PDF favored by academics. This is because text-based media online (generally in HTML or Markdown) has immense accessibility potential over PDF [1] (and paper publications). All artifacts of academic discourse, including every paper publication, should be more accessible. Due to a micro-paper's size, it is easier for authors to learn accessibility for than a full-size paper with proprietary formatting involved. This micro-paper was authored in Microsoft Word, exported as a PDF, and then converted into HTML using pandoc.\\nWith the push for arXiv to transition more towards accessible formats of publication [2], I believe that trustworthy archives that are accessibility-first are near. Micro-papers will compliment this push.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/24', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=402.55999755859375, r=295.6099853515625, b=255.89999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 653))]), DocItem(self_ref='#/texts/25', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=253.00999450683594, r=294.57000732421875, b=209.88999938964844, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 204))])], headings=['3.4 A micro-paper must be accessible'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"If someone has an idea, conversation, or late-breaking work, they might consider the following questions: Why wouldn't I write a short academic paper and submit to a traditional venue?\\nIt is time-consuming, expensive, and requires waiting for the publication cycle. It is also higher risk, in cases where the peer review process might reject it.\\nWhy shouldn't I write a piece in a non-peer reviewed publication, like ACM Interactions?\\nThis is also time-consuming and higher risk, because editorial interest may conflict. In addition, these often require an existing network of colleagues, invitation to contribute, or formal submission and selection process.\\nWhy shouldn't I write a blog or twitter thread? Blogs and social media posts raise concerns about archival quality and trustworthiness. Accessing the piece later may become difficult or cumbersome. Some great ideas and discussions have been lost in time due to the ephemeral nature of these cheap and fast options.\\nWhen is writing a micro-paper a good idea?\\nNotably, there is no formal peer review for a micropaper. Our currently imagined peer review process may not make sense for all work published with the intent to push new ideas and conversations. It is even worth considering if this practice should continue at all [3, 4].\\nA practical use for a micro-paper may be as a preprint or early draft for an eventual short or full paper submission, position paper, or book chapter. The greatest strength of both the pre-print and short paper process is that they can invigorate scholars with new or raw ideas to see those turn into full projects. Short papers also have a core readership and opportunities to present that are not afforded to micro-papers, so for early career researchers it may be important to consider ways to use these two formats together.\\nFor folks who simply want to get a citable idea out into the world without regard for submission and publication cycles and procedures, a micro-paper is a good choice as well.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/27', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=167.47999572753906, r=291.0799865722656, b=124.36799621582031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 184))]), DocItem(self_ref='#/texts/28', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=72.0250015258789, t=121.44999694824219, r=292.875, b=89.84500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 160))]), DocItem(self_ref='#/texts/29', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=717.6599731445312, r=530.8699951171875, b=697.5380249023438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 88))]), DocItem(self_ref='#/texts/30', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=694.6500244140625, r=538.22998046875, b=651.52001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 223))]), DocItem(self_ref='#/texts/31', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=637.1400146484375, r=539.969970703125, b=570.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 314))]), DocItem(self_ref='#/texts/32', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=556.6099853515625, r=495.70001220703125, b=547.9879760742188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 42))]), DocItem(self_ref='#/texts/33', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=545.0999755859375, r=540.5700073242188, b=490.4700012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 272))]), DocItem(self_ref='#/texts/34', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=487.5799865722656, r=540.7000122070312, b=375.45001220703125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 528))]), DocItem(self_ref='#/texts/35', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=372.55999755859375, r=541.969970703125, b=329.4200134277344, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 175))])], headings=['4. Considering when a micro-paper is the write choice (pun intended)'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"And lastly, there may be authors with too many ideas to pursue (even when some are useful) and they are willing to admit that they won't pursue every idea that they have. A micro-paper is a way to put the idea into the discussion and let it run its course. In my case, I recognize that some problems and patterns are outside of the scope of my leverage and experience to address, such as contributions to design or behavioral domains of accessibility (when my area is strictly technical contributions).\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/36', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=326.5299987792969, r=536.969970703125, b=214.39999389648438, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 502))])], headings=['4. Considering when a micro-paper is the write choice (pun intended)'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text='The hope is that both the procedural and systemic inaccessibility of the short paper authoring process and the citational uncertainty of blogs and social media can be addressed with the micro-paper. I hope to see early, usable ideas shared more freely and especially hope to invigorate young scholars and include historically excluded folks, such as those with disabilities, in the larger research conversation.' meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/38', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=2, bbox=BoundingBox(l=315.1300048828125, t=185.72999572753906, r=538.219970703125, b=96.59500122070312, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 411))])], headings=['5. Conclusion'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"Special thanks to Jonathan Zong, for your encouragement to make my cheap ideas citable and for feedback on this micro-paper. Also thanks to the folks at the MIT Vis Lab (Arvind, Crystal, and Alan) for supporting my nottraditionally-publishable ideas over the past couple years especially. Hearing that my 'tweets are a public service' encouraged me to make the heart of that service last longer than Twitter does (hopefully).\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/40', parent=RefItem(cref='#/body'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.TEXT: 'text'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=691.9000244140625, r=532.3699951171875, b=648.77001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 425))])], headings=['Acknowledgements'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n",
      "text=\"[1] Bigham, Jeffrey et al. 'An Uninteresting Tour Through Why Our Research Papers Aren't Accessible.' CHI , May 2016. doi: 10.1145/2851581.2892588.\\n[2] Brinn, Shamsi et al. 'A framework for improving the accessibility of research papers on arXiv.org.' arXiv , Dec. 2022. doi: 10.48550/ARXIV.2212.07286\\nAccessed: Feb. 24, 2023.\\n[3] Mastroianni, Adam. 'The rise and fall of peer review.' Experimental History, https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review\\n[4] Mastroianni, Adam et al. 'Things Could Be Better.' PsyArXiv , Nov 2022.\\n[5] Sanderson, Robert et al. 'Analyzing the Persistence of Referenced Web Resources with Memento.' arXiv , May 2011. doi: 10.48550/arXiv.1105.3459\\n[6] 'Why Do People Publish on Arxiv Instead of Other Places?' Academia Stack Exchange , Accessed: Feb. 23, 2023, https://academia.stackexchange.com/questions/75325/why-do-people-publish-on-arxiv-instead-ofother-places.\\n[7] Zittrain, Jonathan et al. 'Perma: Scoping and Addressing the Problem of Link and Reference Rot in Legal Citations.' Harvard Law Review , Mar. 2014.\" meta=DocMeta(schema_name='docling_core.transforms.chunker.DocMeta', version='1.0.0', doc_items=[DocItem(self_ref='#/texts/42', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=620.1400146484375, r=541.7000122070312, b=600.02001953125, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 147))]), DocItem(self_ref='#/texts/43', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=597.1099853515625, r=537.969970703125, b=576.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 153))]), DocItem(self_ref='#/texts/44', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=397.4200134277344, t=574.0999755859375, r=502.8599853515625, b=565.489990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 24))]), DocItem(self_ref='#/texts/45', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=574.1099853515625, r=407.17999267578125, b=553.989990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 156))]), DocItem(self_ref='#/texts/46', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=551.1099853515625, r=387.6499938964844, b=542.4879760742188, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 75))]), DocItem(self_ref='#/texts/47', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=539.6099853515625, r=535.719970703125, b=519.489990234375, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 146))]), DocItem(self_ref='#/texts/48', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=516.5900268554688, r=521.7000122070312, b=484.9700012207031, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 218))]), DocItem(self_ref='#/texts/49', parent=RefItem(cref='#/groups/0'), children=[], content_layer=<ContentLayer.BODY: 'body'>, label=<DocItemLabel.LIST_ITEM: 'list_item'>, prov=[ProvenanceItem(page_no=3, bbox=BoundingBox(l=72.0250015258789, t=482.0799865722656, r=512.4099731445312, b=461.9679870605469, coord_origin=<CoordOrigin.BOTTOMLEFT: 'BOTTOMLEFT'>), charspan=(0, 151))])], headings=['References'], captions=None, origin=DocumentOrigin(mimetype='application/pdf', binary_hash=5309462489565315625, filename='4c32337a0bb1d5c6faa1210de6fdf4b5', uri=None))\n"
     ]
    }
   ],
   "source": [
    "# for page_num, page in enumerate(doc.pages):\n",
    "#     chunks = page.chunk()  # Split into smaller parts\n",
    "\n",
    "#     for chunk_idx, chunk in enumerate(chunks):\n",
    "#         embedding = doc.embed(chunk.text)\n",
    "\n",
    "#         # Insert into semantic_search\n",
    "#         cur.execute(\"\"\"\n",
    "#             INSERT INTO semantic_search (id, page, chunk, embedding)\n",
    "#             VALUES (%s, %s, %s, %s)\n",
    "#         \"\"\", (doc_id, page_num + 1, chunk_idx, embedding))\n",
    "\n",
    "#         # Insert into keyword_search\n",
    "#         cur.execute(\"\"\"\n",
    "#             INSERT INTO keyword_search (id, page, ts)\n",
    "#             VALUES (%s, %s, to_tsvector('english', %s))\n",
    "#         \"\"\", (doc_id, page_num + 1, chunk.text))\n",
    "\n",
    "# conn.commit()\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ab7aa-8e54-4cb7-9be0-5946932ef720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
