{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6159ba07-aab4-4177-8883-ed51d59e69ca",
   "metadata": {},
   "source": [
    "# Combined data flow use case\n",
    "\n",
    "This notebook works through the process for loading PDF files into a vector database and performing a semantic search over those files. This involves two data flows:\n",
    "1. Processing the PDF files and loading the embedded results into a vector database.\n",
    "2. Executing the search against a vector index and returning the most semantically similar results.\n",
    "\n",
    "Since Denodo can manage both the unstructured data and the access to the vector database, we can streamline both of these processes:\n",
    "* Denodo can serve the PDF files to an application that can process, chunk, and embed the PDF files.\n",
    "* Denodo can load the vector database with the results generated by the application.\n",
    "* Applications wanting to perform a vector search can access the Denodo Platform to do this as well, using simple SQL statements.\n",
    "\n",
    "As usual, we'll download the required packages first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc098fe-d406-4c35-b9ee-83ab4cf797d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_core langchain_community langchain-openai pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae0e83-582d-4b4e-9a97-5d3b138e2ec6",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Here we're importing a few helper libraries. The ones to note are the PyMuPDFLoader, which is important for its ability to process PDF into text. The LangChain RecursiveCharacterTextSplitter will be used to split the full pages of PDF text into smaller chunks, so that their semantic meaning is not diluted when being embedded by our embeddings model.\n",
    "\n",
    "Finally, we also import the DenodoVector library to help us interact more efficiently with the vector database through the Denodo Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90220ac1-741e-4ad4-af97-2140d46df0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import base64, os, urllib.parse, json, re\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We have to move up a directory to import the Denodo libraries, but we switch back to avoid issues.\n",
    "%cd ..\n",
    "try:\n",
    "    from denodo_python.denodo_vectorstore import DenodoVector, create_denodo_connection, denodo_connection_param, denodo_connection_param_oauth, underlying_view_exists\n",
    "except Exception as err:\n",
    "    print(f\"Could not import helper libraries, please check with the support team: {err}\")\n",
    "finally:\n",
    "    %cd ./1_denodo_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2c382-f1be-4833-8034-2ec59ad99e7b",
   "metadata": {},
   "source": [
    "## Instantiating Embedding Model\n",
    "\n",
    "Here we use the LangChain BedrockEmbeddings class to interact with Bedrock's embeddings models. This model can be fed into a VectorStore implementation in order to automatically handle embedding operations before inserting the results into the vector database.\n",
    "\n",
    "In order to check that it's working correctly, we also call it on an individual string `Hello` to make sure that we can get a list of float values back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02bb1a-ce15-4804-adfe-5f78f8c9f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our normal embeddings model, that will be used to generate vector embeddings from the input data\n",
    "# We need this for our LangChain VectorStore.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "embed_text = embeddings.embed_query('Hello')\n",
    "print(str(embed_text[0:3]) + f\" + {len(embed_text) - 3} more float values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0165540-e32b-40dc-9264-21692dfed5de",
   "metadata": {},
   "source": [
    "## Denodo Connection\n",
    "\n",
    "Here we connect to the Denodo Platform with the Flight SQL driver and retrieve PDF files from the unstructured data store. Note that we can select specific PDF files by specifying the extension and relative path of these files in the underlying data source.\n",
    "\n",
    "Please make sure to replace the value of `host` with the DNS name of the EC2 instance hosting the Denodo Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7141be-7da3-4a4f-91eb-71b9323af47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Update this to point to the EC2 instance's DNS name or we won't be able to access Denodo !!! Example:\n",
    "# host=\"ec2-3-238-31-20.compute-1.amazonaws.com\"\n",
    "host=\"denodo-service\"\n",
    "\n",
    "# Define connection parameters to the Denodo Platform so that we can connect.\n",
    "port = 9994\n",
    "db = \"vector\"\n",
    "user = \"admin\"\n",
    "password = \"admin\"\n",
    "\n",
    "# The following function from the Denodo library constructs connection parameters that we can feed into another function\n",
    "# To create the connection. For OAuth support we have the \"denodo_connection_param_oauth\" function.\n",
    "denodo_con_param = denodo_connection_param(user, password, host, db, port)\n",
    "\n",
    "# This query retreives our PDF documents. Note that retrieving different files is as simple as updating the WHERE clause.\n",
    "with create_denodo_connection(denodo_con_param) as con:\n",
    "    with con.cursor() as cur:\n",
    "        cur.execute(\"SELECT blob_value, file_name, uri FROM bank.bv_local_files WHERE extension = 'pdf'\")\n",
    "        pdf_results = cur.fetchallarrow()\n",
    "\n",
    "for name in pdf_results.column(1):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f9548-848d-41c4-b22e-314a76d93b72",
   "metadata": {},
   "source": [
    "## PDF Processing\n",
    "\n",
    "After getting the actual binary values for our PDFs, we still have to feed them into a library that can convert the binary data into human readable text and associated metadata--this is what the PyMuPDFLoader is able to do for us.\n",
    "\n",
    "After loading the PDF, LangChain takes over in processing the text into a format that will work well for our vector index--this is performed by the RecursiveCharacterTextSplitter that we can configure to generate chunks of text that are small enough to have a specific semantic meaning. Finally, we're left with a list of `Document` objects that we can feed into our `VectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198be6d-b2c0-489f-ab5f-07b7bc20a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a vector search to work well, we want to split our text into smaller chunks, so that too many sentences with different contexts\n",
    "# are not included in the same chunk. To achieve this, we use a LangChain text splitter.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# To extract text from the PDF files, we use the PyPDFLoader. \n",
    "pdfs = []\n",
    "for blob_value, file_name, uri in zip(pdf_results.column(0),pdf_results.column(1),pdf_results.column(2)):\n",
    "    # The PDF loader requires a file path, so we temporarily save the PDF to the local filesystem\n",
    "    tmp_file_path = '/tmp/output_file.bin'\n",
    "    with open(tmp_file_path, 'wb') as file:\n",
    "        file.write(blob_value.as_py())\n",
    "        loader = PyMuPDFLoader(tmp_file_path)\n",
    "        pdf = loader.load()\n",
    "        # The loader automatically sets the metadata using the file location, however, we want to overwrite this with how to find the file in Denodo.\n",
    "        for p in pdf:\n",
    "            page = str(int(p.metadata['page']) + 1)\n",
    "            # URI field is not being properly set. This should be resolved\n",
    "            p.metadata = {\"source\": uri.as_py(), \"page\": page, \"link\": f\"http://{host}:9090/denodo-restfulws/unstructured/views/all_files/{urllib.parse.quote(uri.as_py()).replace('/', '%2F')}?%24select=blob_value#page={page}\"}\n",
    "        pdfs += pdf\n",
    "        print(f\"Parsed document '{file_name.as_py()}'\")\n",
    "    if os.path.exists(tmp_file_path):\n",
    "        os.remove(tmp_file_path)\n",
    "    else:\n",
    "        print(f\"Error: {tmp_file_path} does not exist.\")\n",
    "\n",
    "# Concatenate all of our text chunks\n",
    "pages = []\n",
    "pages += text_splitter.split_documents(pdfs)\n",
    "print(f\"Total text chunks: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942037f1-5816-4e4d-80d7-888acdaed829",
   "metadata": {},
   "source": [
    "## Creating the `VectorStore` Object\n",
    "\n",
    "In this case, instead of results, we're feeding in a list of LangChain `Document` objects to the vector store--these contain metadata about where the `Document` originated along with the actual text contained in the `Document`. These objects are the common output of LangChain processes.\n",
    "\n",
    "Note that this does take a while to embed all of the data (~2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44a191-7131-42f6-8ecd-68c31107bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the collection into which we will be inserting our embeddings and text\n",
    "collection_name = \"pdf_vector_search\"\n",
    "\n",
    "# This command ensures that any existing table with the same name is deleted, using helper functions\n",
    "if 'pdf_denodo_vec' in locals():\n",
    "    pdf_denodo_vec.delete_index()\n",
    "    print(\"Deleted existing VectorStore\")\n",
    "with create_denodo_connection(denodo_con_param) as con:\n",
    "    with con.cursor() as cur:\n",
    "        if underlying_view_exists(cur,denodo_con_param['db'],collection_name):\n",
    "            cur.execute(f\"SELECT * FROM DROP_REMOTE_TABLE() WHERE base_view_name = '{collection_name}'\")\n",
    "            cur.fetchone()\n",
    "            print(\"Deleted existing backend table\")\n",
    "\n",
    "# This is inserting the text chunks into PGVector after embedding them. It's a bit slow but it should be possible to adapt this to a PySpark pipeline to parallelize it. \n",
    "\n",
    "# Initialize the DenodoVector VectorStore\n",
    "pdf_denodo_vec = DenodoVector.from_documents(\n",
    "    documents=pages,\n",
    "    embedding=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_param=denodo_con_param,\n",
    "    # These variables let the Python library know where the vector database data source is defined in Denodo\n",
    "    db_db='admin', \n",
    "    db_name='ds_pgvector', \n",
    "    db_schema = 'public'\n",
    ")\n",
    "\n",
    "# This will take a bit to insert all the records, since pyodbc inserts them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff51207-b803-47de-aa67-5c3aba69ba9e",
   "metadata": {},
   "source": [
    "## Vector Search\n",
    "\n",
    "After creating a `VectorStore` object, performing a semantic search against the data is as easy as calling the single `similarity_search()` function on that object with the string being searched for.\n",
    "\n",
    "Note that the response metadata also contains a link through which the PDF file itself can be accessed, and this automatically navigates to the page containing the search result using a URL parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15c201-4553-45a6-bad7-a9f9d4eff6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This executes a similarity search against our vector store, which returns a list of documents\n",
    "output = pdf_denodo_vec.similarity_search(\"What risks does Microsoft highlight in their report?\")\n",
    "\n",
    "# I would like each document to be led by its location in Denodo, so that I can drill into the document if necessary\n",
    "# Clicking on the link included in the source will open the document in the browser, accessing it through a Denodo web service\n",
    "for doc in output:\n",
    "    print(f\"\"\"Source: {doc.metadata}\n",
    "Content: {doc.page_content}\n",
    "---\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8184e2-b4bb-46f4-956f-830175dbcc82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
